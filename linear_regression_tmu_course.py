# -*- coding: utf-8 -*-
"""Linear Regression-TMU Course.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xN-Nh7rDzTbgcO1Oa0QI_S1drOYm2uDi
"""

import numpy as np
import matplotlib.pyplot as plt

"""**Creating Dataset**"""

X = np.linspace(0, 50, 20)[:, np.newaxis]
y = 15 + 1.2 * X + np.random.normal(0, 10, size=X.shape)
print(X.shape, y.shape)

"""**Hypothesis Function**"""

def h(X, w, has_bias=False):
  if has_bias:
    return np.dot(X, w)
  else:
    X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
    return np.dot(X, w)

w = np.array([[1], [5]])
print(h(X, w))

"""**show function**
 
for visualizing data and hypothesis
"""

def show(X, y, w):
  outputs = h(X, w)
  plt.figure()
  plt.plot(X, y, 'rx', X, outputs, 'b--')
  plt.show()

show(X, y, w)

"""**Cost Function**

$ J = \frac{1}{2m} \sum_{i=1}^{m} (y^{(i)} - h(x^{(i)}))^2$

"""

def J(X, y, w):
  outputs = h(X, w)
  errors = y - outputs
  return np.dot(errors.T, errors)[0, 0] / (2 * X.shape[0])

print(J(X, y, w))

"""**Training Step**"""

def train_step(X, y, w, lr=0.001):
  outputs = h(X, w)
  errors = y - outputs
  X = np.concatenate((np.ones((X.shape[0], 1)) , X), axis=1)
  w = w + lr * np.dot(X.T, errors) / X.shape[0]
  return w

w = np.array([[0], [0.]])
w = train_step(X, y, w)
print(w)

"""**fit function**"""

def fit(X, y, w0, lr=0.001, max_iters=1000, verbose=False):
  w = w0
  if verbose:
    print(f'Iteration 0: J = {J(X, y, w)}')
  for i in range(max_iters):
    w = train_step(X, y, w, lr)
    show(X, y, w)
    if verbose:
      print(f'Iteration {i + 1}: J = {J(X, y, w)}')
  return w

w = np.array([[0], [0.]])
show(X, y, w)
w = fit(X, y, w, max_iters=10, verbose=1)
show(X, y, w)

"""**Normal Equation**

It is better than gradient descent method because you get the minimum in a few computations, instead of maybe thousands of iterations.

**Finding Appropriate Weights**
"""

def find_weights(X, y):
  X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)
  w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y)
  return w

w = find_weights(X, y)
print(w)
show(X, y, w)